{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9388/2031286696.py:17: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=driver_path, options=options)\n"
     ]
    }
   ],
   "source": [
    "# DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
    "driver_path = '/usr/bin/chromedriver'\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-gpu')\n",
    "\n",
    "# Headless run option for later use\n",
    "# chrome_options.add_argument('--headless')\n",
    "\n",
    "# Media query = desktop \n",
    "options.add_argument('--window-size=1600,900')\n",
    "\n",
    "# Fixes 'data:,' url issue \n",
    "options.add_argument('--remote-debugging-port=9225')\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=driver_path, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open website URL\n",
    "url = 'https://korean.visitkorea.or.kr/main/main.do#home'\n",
    "driver.get(url)\n",
    "\n",
    "# Page loading buffer\n",
    "time.sleep(2)\n",
    "\n",
    "# Ask search keyword\n",
    "# search_keyword = input('Enter search keyword: ')\n",
    "search_keyword = \"서울\"\n",
    "\n",
    "# Ask file name and path\n",
    "# save_file = input('Enter full CSV file path and name: ')\n",
    "file_path = \"/home/linux-user/Downloads/dataset.csv\"\n",
    "log_path = \"/home/linux-user/Downloads/log.txt\"\n",
    "\n",
    "# Locate search button\n",
    "# Use attributes instead of 'driver.find_element_by_class_name' method\n",
    "driver.find_element(By.CSS_SELECTOR, '.btn_search').click()\n",
    "\n",
    "# Locate search box\n",
    "# 'element = driver.find_element(By.CSS_SELECTOR, '#inp_search').click()' returns none, seperate the action\n",
    "element = driver.find_element(By.CSS_SELECTOR, '#inp_search')\n",
    "element.click()\n",
    "\n",
    "# Send query keyword to search box\n",
    "# 'element.sendkeys(search_keyword)' is deprecated\n",
    "element.send_keys(search_keyword)\n",
    "\n",
    "# Perform search \n",
    "driver.find_element(By.CSS_SELECTOR, \".btn_search\").click()\n",
    "\n",
    "# Page loading buffer until delayed popup appears\n",
    "time.sleep(15)\n",
    "\n",
    "# Disable fullscreen popup\n",
    "driver.find_element(By.CSS_SELECTOR, \"._sa_t-size__medium\").click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to save log as file\n",
    "log = sys.stdout\n",
    "f = open(log_path, 'a', encoding='UTF-8')\n",
    "sys.stdout = f\n",
    "\n",
    "# Announce the process\n",
    "now = datetime.now()\n",
    "current_datetime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(\"The script is activated.\")\n",
    "print(\"Current time: {}\".format(current_datetime))\n",
    "\n",
    "# Initiate bs4 html.parser to read the whole HTML\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Exclude ad banner area from first page\n",
    "unwanted = soup.select_one('li.banner_area')\n",
    "unwanted.extract()\n",
    "\n",
    "# Collect search results\n",
    "search_result = soup.find('ul', class_='list_thumType')\n",
    "\n",
    "# Define data arrays\n",
    "title_list = []\n",
    "region_list = []\n",
    "hashtag_list = []\n",
    "\n",
    "# Check the last page\n",
    "current_page = 1\n",
    "page_box = driver.find_element(By.ID, \"page_box\")\n",
    "pages = page_box.find_elements(By.TAG_NAME, \"a\")\n",
    "last_page = int(pages[-1].get_attribute('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start page turner\n",
    "try:\n",
    "    while current_page < last_page:\n",
    "        if current_page == 1:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            # Exclude ad banner area from first page\n",
    "            unwanted = soup.select_one('li.banner_area')\n",
    "            unwanted.extract()  \n",
    "            search_result = soup.find('ul', class_='list_thumType')\n",
    "            for i in search_result:\n",
    "                title = i.select_one(\"div.tit\").get_text()\n",
    "                title_list.append(title)\n",
    "                try:\n",
    "                    region = i.select_one(\"p\").get_text()\n",
    "                    region_list.append(region)                    \n",
    "                except AttributeError:\n",
    "                    region = \"N/A\"\n",
    "                    region_list.append(region)\n",
    "                try:    \n",
    "                    hashtag = i.select_one(\"p.tag_type\").get_text()\n",
    "                    hashtag_list.append(hashtag)                    \n",
    "                except AttributeError:\n",
    "                    hashtag = \"N/A\"\n",
    "                    hashtag_list.append(hashtag)\n",
    "            current_page += 1\n",
    "            current_page_str = str(current_page)\n",
    "            next_page = driver.find_element(By.LINK_TEXT, current_page_str)\n",
    "            next_page.send_keys(Keys.ENTER)\n",
    "            time.sleep(7)\n",
    "        elif current_page % 5 != 0:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            search_result = soup.find('ul', class_='list_thumType')\n",
    "            for i in search_result:\n",
    "                title = i.select_one(\"div.tit\").get_text()\n",
    "                title_list.append(title)\n",
    "                try:\n",
    "                    region = i.select_one(\"p\").get_text()\n",
    "                    region_list.append(region)                    \n",
    "                except AttributeError:\n",
    "                    region = \"N/A\"\n",
    "                    region_list.append(region)\n",
    "                try:    \n",
    "                    hashtag = i.select_one(\"p.tag_type\").get_text()\n",
    "                    hashtag_list.append(hashtag)                    \n",
    "                except AttributeError:\n",
    "                    hashtag = \"N/A\"\n",
    "                    hashtag_list.append(hashtag)\n",
    "            current_page += 1\n",
    "            current_page_str = str(current_page)\n",
    "            next_page = driver.find_element(By.LINK_TEXT, current_page_str)\n",
    "            next_page.send_keys(Keys.ENTER)\n",
    "            time.sleep(7)        \n",
    "        elif current_page % 5 == 0:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            search_result = soup.find('ul', class_='list_thumType')\n",
    "            for i in search_result:\n",
    "                title = i.select_one(\"div.tit\").get_text()\n",
    "                title_list.append(title)\n",
    "                try:\n",
    "                    region = i.select_one(\"p\").get_text()\n",
    "                    region_list.append(region)                    \n",
    "                except AttributeError:\n",
    "                    region = \"N/A\"\n",
    "                    region_list.append(region)\n",
    "                try:    \n",
    "                    hashtag = i.select_one(\"p.tag_type\").get_text()\n",
    "                    hashtag_list.append(hashtag)                    \n",
    "                except AttributeError:\n",
    "                    hashtag = \"N/A\"\n",
    "                    hashtag_list.append(hashtag)\n",
    "            current_page += 1\n",
    "            current_page_str = str(current_page)\n",
    "            next_page = driver.find_element(By.CSS_SELECTOR, \".btn_next\")\n",
    "            next_page.send_keys(Keys.ENTER)\n",
    "            time.sleep(7)\n",
    "        elif current_page == last_page:\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            search_result = soup.find('ul', class_='list_thumType')\n",
    "            for i in search_result:\n",
    "                title = i.select_one(\"div.tit\").get_text()\n",
    "                title_list.append(title)\n",
    "                try:\n",
    "                    region = i.select_one(\"p\").get_text()\n",
    "                    region_list.append(region)                    \n",
    "                except AttributeError:\n",
    "                    region = \"N/A\"\n",
    "                    region_list.append(region)\n",
    "                try:    \n",
    "                    hashtag = i.select_one(\"p.tag_type\").get_text()\n",
    "                    hashtag_list.append(hashtag)                    \n",
    "                except AttributeError:\n",
    "                    hashtag = \"N/A\"\n",
    "                    hashtag_list.append(hashtag)\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    now = datetime.now()\n",
    "    current_datetime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(\"An error has occured. \", e)\n",
    "    print(\"Current time: {}\".format(current_datetime))\n",
    "\n",
    "finally:\n",
    "    # Initiate Pandas dataframe\n",
    "    dataset = pd.DataFrame()\n",
    "    dataset['Title'] = title_list\n",
    "    dataset['Region'] = region_list\n",
    "    dataset['Hashtags'] = hashtag_list\n",
    "    dataset['Hashtags'].df.replace('#', ' ', inplace=True)\n",
    "\n",
    "    # Save dataset as CSV file\n",
    "    dataset.to_csv(file_path, encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_datetime = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(\"The script has been finished successfully.\")\n",
    "    print(\"Current time: {}\".format(current_datetime))\n",
    "\n",
    "    # Save log as TXT file\n",
    "    sys.stdout = log\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
